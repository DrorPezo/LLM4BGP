{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "662fac50"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langgraph langchain langchain-openai\n",
        "%pip install -qU langchain-tavily\n",
        "!pip install langchain-groq\n",
        "!pip install -qU langchain-anthropic\n",
        "!pip install neo4j\n",
        "!pip install pyvis\n",
        "!pip install pycountry\n",
        "!pip install ipaddress"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mounting Google Drive**"
      ],
      "metadata": {
        "id": "Tk0fkkUNqbYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/MyDrive/LLM4BGP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2Nt1dqJLrqE",
        "outputId": "437ccc4c-3334-4135-85eb-7d60fcb3497e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/LLM4BGP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BGPStream**"
      ],
      "metadata": {
        "id": "1oB9_hhGno18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PATH\"] += \":/root/.cargo/bin\""
      ],
      "metadata": {
        "id": "KSsM8hHYnzbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
        "cargo install monocle\n",
        "sudo apt-get update\n",
        "sudo apt-get install -y curl apt-transport-https ssl-cert ca-certificates gnupg lsb-release\n",
        "curl -1sLf 'https://dl.cloudsmith.io/public/wand/libwandio/cfg/setup/bash.deb.sh' | sudo -E bash\n",
        "echo \"deb https://pkg.caida.org/os/$(lsb_release -si|awk '{print tolower($0)}') $(lsb_release -sc) main\" | sudo tee /etc/apt/sources.list.d/caida.list\n",
        "sudo wget -O /etc/apt/trusted.gpg.d/caida.gpg https://pkg.caida.org/os/ubuntu/keyring.gpg\n",
        "sudo apt update; sudo apt-get install bgpstream"
      ],
      "metadata": {
        "id": "e_H_SqQan3eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pybgpstream\n",
        "!pip install prsw\n",
        "!python3 -m pip install pybgpkit-parser\n",
        "!python3 -m pip install pybgpkit"
      ],
      "metadata": {
        "id": "nSHpg-Kdn7Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import time\n",
        "import datetime as dt\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from typing import Dict, Iterable, Iterator, List, Optional, Set, Tuple\n",
        "import pybgpstream\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "nVT1iLECec3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tools for Solving Problems Without the Need of AS Graph**"
      ],
      "metadata": {
        "id": "_hs91tFLzd1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def utc_now() -> int:\n",
        "    \"\"\"Return current UNIX epoch time (UTC, seconds).\"\"\"\n",
        "    return int(time.time())\n",
        "\n",
        "\n",
        "def window_last_minutes(minutes: int) -> Tuple[int, int]:\n",
        "    \"\"\"Return (start_ts, end_ts) for the last *minutes* minutes.\n",
        "\n",
        "    PyBGPStream expects epoch seconds.\n",
        "    \"\"\"\n",
        "    end = utc_now()\n",
        "    start = end - minutes * 60\n",
        "    return start, end\n",
        "\n",
        "\n",
        "def ts_str(ts: Optional[int]) -> str:\n",
        "    \"\"\"Human‑readable UTC timestamp string for display.\"\"\"\n",
        "    if ts is None:\n",
        "        return \"—\"\n",
        "    return dt.datetime.utcfromtimestamp(ts).strftime(\"%Y-%m-%d %H:%M:%SZ\")"
      ],
      "metadata": {
        "id": "lNgES2M908EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def origin_asns(as_path: str) -> Set[int]:\n",
        "    _ASN_RE = re.compile(r\"\\d+\")\n",
        "    if not as_path:\n",
        "        return set()\n",
        "    tail = as_path.strip().split()[-1]\n",
        "    tail_nums = {int(x) for x in _ASN_RE.findall(tail)}\n",
        "    if tail_nums:\n",
        "        return tail_nums\n",
        "    # Fallback: any numbers in the full path\n",
        "    all_nums = _ASN_RE.findall(as_path)\n",
        "    return {int(all_nums[-1])} if all_nums else set()"
      ],
      "metadata": {
        "id": "9IYF7jE108-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_PROJECTS = [\"ris\", \"routeviews\"]\n",
        "\n",
        "def make_stream(\n",
        "    prefix: str,\n",
        "    start_ts: int,\n",
        "    end_ts: int,\n",
        "    *,\n",
        "    projects: Optional[List[str]] = None,\n",
        "    collectors: Optional[List[str]] = None,\n",
        "    record_type: str = \"updates\",\n",
        "    more_specifics: bool = True,\n",
        ") -> pybgpstream.BGPStream:\n",
        "    \"\"\"Create a configured BGPStream for a prefix family in a time window.\n",
        "\n",
        "    Notes:\n",
        "      • We use projects RIS + RouteViews (closest to \"RIS Live + RouteViews\" in\n",
        "        BGPStream naming). Using record_type='updates' and a recent 10‑minute\n",
        "        window effectively approximates the recent/live view.\n",
        "      • Filter syntax: 'prefix more X' includes the exact prefix and more‑specifics.\n",
        "    \"\"\"\n",
        "    filt = f\"prefix {'more ' if more_specifics else ''}{prefix}\"\n",
        "    kwargs = dict(\n",
        "        from_time=start_ts,\n",
        "        until_time=end_ts,\n",
        "        record_type=record_type,\n",
        "        filter=filt,\n",
        "    )\n",
        "    if projects is None:\n",
        "        projects = DEFAULT_PROJECTS\n",
        "    if projects:\n",
        "        kwargs[\"projects\"] = projects\n",
        "    if collectors:\n",
        "        kwargs[\"collectors\"] = collectors\n",
        "    return pybgpstream.BGPStream(**kwargs)"
      ],
      "metadata": {
        "id": "v8H8I6bk1Avn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iter_bgp_elements(stream: pybgpstream.BGPStream) -> Iterator[dict]:\n",
        "    \"\"\"Yield normalized dicts for announcement-containing elements.\n",
        "\n",
        "    Each yielded item includes: project, collector, record_time, elem_time,\n",
        "    type, peer_asn, peer_address, prefix, as_path.\n",
        "    \"\"\"\n",
        "    for rec in stream.records():\n",
        "        if getattr(rec, \"status\", None) not in (None, \"valid\"):\n",
        "            continue\n",
        "        common = dict(project=rec.project, collector=rec.collector, record_time=rec.time)\n",
        "        for elem in rec:\n",
        "            etype = elem.type  # 'A' (announcement), 'W' (withdrawal), 'R' (RIB)\n",
        "            if etype not in (\"A\", \"R\"):\n",
        "                continue\n",
        "            f = elem.fields\n",
        "            prefix = f.get(\"prefix\") or f.get(\"announced-prefix\")\n",
        "            as_path = f.get(\"as-path\", \"\")\n",
        "            yield dict(\n",
        "                **common,\n",
        "                elem_time=elem.time,\n",
        "                type=etype,\n",
        "                peer_asn=int(elem.peer_asn) if elem.peer_asn is not None else None,\n",
        "                peer_address=elem.peer_address,\n",
        "                prefix=prefix,\n",
        "                as_path=as_path,\n",
        "            )"
      ],
      "metadata": {
        "id": "PxJ_CbLo4_h2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Vantage = Tuple[str, str, Optional[int]]  # (project, collector, peer_asn)\n",
        "\n",
        "def build_vantage_origin_index(elems: Iterable[dict]):\n",
        "    \"\"\"Return nested mapping:\n",
        "        { (project, collector, peer_asn):\n",
        "            { prefix:\n",
        "                { origin_asn:\n",
        "                    { 'first': ts, 'last': ts, 'count': n }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    \"\"\"\n",
        "    idx: Dict[Vantage, Dict[str, Dict[int, Dict[str, int]]]] = defaultdict(\n",
        "        lambda: defaultdict(lambda: defaultdict(lambda: {\"first\": None, \"last\": None, \"count\": 0}))\n",
        "    )\n",
        "    for e in elems:\n",
        "        pfx = e.get(\"prefix\")\n",
        "        oset = origin_asns(e.get(\"as_path\", \"\"))\n",
        "        if not pfx or not oset:\n",
        "            continue\n",
        "        v: Vantage = (e[\"project\"], e[\"collector\"], e[\"peer_asn\"])\n",
        "        ts = e[\"elem_time\"]\n",
        "        bucket = idx[v][pfx]\n",
        "        for o in oset:\n",
        "            meta = bucket[o]\n",
        "            meta[\"first\"] = ts if meta[\"first\"] is None else min(meta[\"first\"], ts)\n",
        "            meta[\"last\"] = ts if meta[\"last\"] is None else max(meta[\"last\"], ts)\n",
        "            meta[\"count\"] += 1\n",
        "    return idx"
      ],
      "metadata": {
        "id": "YvF3bE691AzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_conflicts_by_vantage(index) -> Dict[Vantage, Dict[str, Dict[int, dict]]]:\n",
        "    \"\"\"Filter to vantages where at least one prefix has >1 origin ASNs.\"\"\"\n",
        "    out: Dict[Vantage, Dict[str, Dict[int, dict]]] = {}\n",
        "    for v, pmap in index.items():\n",
        "        conflicted = {pfx: omap for pfx, omap in pmap.items() if len(omap) > 1}\n",
        "        if conflicted:\n",
        "            out[v] = conflicted\n",
        "    return out\n",
        "\n",
        "\n",
        "def aggregate_family_origins(index) -> Set[int]:\n",
        "    \"\"\"All distinct origins observed for the prefix family (any vantage/prefix).\"\"\"\n",
        "    s: Set[int] = set()\n",
        "    for pmap in index.values():\n",
        "        for omap in pmap.values():\n",
        "            s.update(omap.keys())\n",
        "    return s"
      ],
      "metadata": {
        "id": "kKz1XMd51JMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_conflicts_table(conflicts: Dict[Vantage, Dict[str, Dict[int, dict]]]):\n",
        "    \"\"\"Return a pandas.DataFrame summarizing conflicts per vantage (if pandas available).\"\"\"\n",
        "    if pd is None:\n",
        "        return None\n",
        "    rows = []\n",
        "    for (project, collector, peer_asn), pmap in conflicts.items():\n",
        "        all_origins: Set[int] = set()\n",
        "        for omap in pmap.values():\n",
        "            all_origins.update(omap.keys())\n",
        "        rows.append(\n",
        "            dict(\n",
        "                project=project,\n",
        "                collector=collector,\n",
        "                peer_asn=peer_asn,\n",
        "                num_conflicting_prefixes=len(pmap),\n",
        "                origins=sorted(all_origins),\n",
        "                prefixes=sorted(pmap.keys()),\n",
        "            )\n",
        "        )\n",
        "    if not rows:\n",
        "        return pd.DataFrame(columns=[\"project\", \"collector\", \"peer_asn\", \"num_conflicting_prefixes\", \"origins\", \"prefixes\"])  # type: ignore\n",
        "    return pd.DataFrame(rows).sort_values([\"num_conflicting_prefixes\", \"collector\"], ascending=[False, True])  # type: ignore"
      ],
      "metadata": {
        "id": "E-1s3dzt1JOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_conflicts(conflicts: Dict[Vantage, Dict[str, Dict[int, dict]]], *, start_ts: int, end_ts: int) -> None:\n",
        "    print(f\"Time window: {ts_str(start_ts)} → {ts_str(end_ts)} (UTC)\")\n",
        "    if not conflicts:\n",
        "        print(\"No origin conflicts observed for the given family and window.\")\n",
        "        return\n",
        "    for (project, collector, peer_asn) in sorted(conflicts.keys()):\n",
        "        pmap = conflicts[(project, collector, peer_asn)]\n",
        "        print(f\"\\nVantage: {project}/{collector}  peer_asn={peer_asn}\")\n",
        "        for pfx in sorted(pmap.keys()):\n",
        "            omap = pmap[pfx]\n",
        "            olist = sorted(omap.items(), key=lambda kv: kv[0])\n",
        "            print(f\"  {pfx} → origins: {', '.join(str(o) for o, _ in olist)}\")\n",
        "            for o, meta in olist:\n",
        "                print(\n",
        "                    f\"    AS{o}: first={ts_str(meta['first'])}, last={ts_str(meta['last'])}, count={meta['count']}\"\n",
        "                )"
      ],
      "metadata": {
        "id": "DSH8h6Mu4Mg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conflicting_origins_by_vantage(\n",
        "    prefix: str,\n",
        "    window_min: int = 10,\n",
        "    *,\n",
        "    projects: Optional[List[str]] = None,  # default RIS + RouteViews\n",
        "    collectors: Optional[List[str]] = None,\n",
        "    record_type: str = \"updates\",\n",
        "    more_specifics: bool = True,\n",
        "):\n",
        "    \"\"\"Compute conflicts for *prefix* (and optionally more‑specifics) in the last *window_min* minutes.\n",
        "\n",
        "    Returns: (conflicts, index, (start_ts, end_ts), family_origins)\n",
        "      - conflicts: vantages → {prefix → {origin_asn → meta}}\n",
        "      - index: full vantage index (including non‑conflicting entries)\n",
        "      - time window: (start_ts, end_ts)\n",
        "      - family_origins: set of all origin ASNs seen across the family\n",
        "    \"\"\"\n",
        "    start_ts, end_ts = window_last_minutes(window_min)\n",
        "    stream = make_stream(\n",
        "        prefix,\n",
        "        start_ts,\n",
        "        end_ts,\n",
        "        projects=projects,\n",
        "        collectors=collectors,\n",
        "        record_type=record_type,\n",
        "        more_specifics=more_specifics,\n",
        "    )\n",
        "    elems = iter_bgp_elements(stream)\n",
        "    index = build_vantage_origin_index(elems)\n",
        "    conflicts = find_conflicts_by_vantage(index)\n",
        "    fam_origins = aggregate_family_origins(index)\n",
        "    return conflicts, index, (start_ts, end_ts), fam_origins"
      ],
      "metadata": {
        "id": "gmvE0xo11JQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------ Preset collector lists (broad coverage) ------------\n",
        "RIS_COLLECTORS = [\n",
        "    \"rrc00\",\"rrc01\",\"rrc03\",\"rrc04\",\"rrc05\",\"rrc06\",\"rrc07\",\n",
        "    \"rrc10\",\"rrc11\",\"rrc12\",\"rrc13\",\"rrc14\",\"rrc15\",\"rrc18\",\n",
        "    \"rrc19\",\"rrc20\",\"rrc21\",\"rrc22\",\"rrc23\",\"rrc24\",\"rrc25\",\"rrc26\",\n",
        "]\n",
        "RV_COLLECTORS = [\n",
        "    \"route-views2\",\"route-views.eqix\",\"route-views.linx\",\"route-views.chicago\",\n",
        "    \"route-views.isc\",\"route-views.kixp\",\"route-views.jinx\",\"route-views.nwax\",\n",
        "    \"route-views.sg\",\"route-views.sydney\",\"route-views.saopaulo\",\"route-views.saopaulo2\",\n",
        "    \"route-views.sfmix\",\"route-views.flix\",\"route-views.perth\",\"route-views.napafrica\",\n",
        "    \"route-views.amsix\",\"route-views.sfmix\",\"route-views.sfmix2\"\n",
        "]\n",
        "\n",
        "\n",
        "def _utc_now() -> int:\n",
        "    return int(time.time())\n",
        "\n",
        "def _window_last_minutes(minutes: int) -> tuple[int, int]:\n",
        "    end = _utc_now()\n",
        "    return end - minutes * 60, end\n",
        "\n",
        "def _fmt_ts(ts: int) -> str:\n",
        "    return dt.datetime.utcfromtimestamp(ts).strftime(\"%Y-%m-%d %H:%M:%SZ\")\n",
        "\n",
        "\n",
        "def print_elements(\n",
        "    prefix: str,\n",
        "    minutes: int = 15,\n",
        "    *,\n",
        "    projects: Optional[List[str]] = None,     # e.g., [\"ris\"], [\"routeviews\"], or both\n",
        "    collectors: Optional[List[str]] = None,\n",
        "    include_withdrawals: bool = False,\n",
        "    more_specifics: bool = True,\n",
        "    record_type: str = \"updates\",            # \"updates\" or \"ribs\"\n",
        ") -> int:\n",
        "    \"\"\"Print normalized lines for recent BGP elements matching the prefix family.\n",
        "    Returns number of printed lines.\n",
        "    \"\"\"\n",
        "    start_ts, end_ts = _window_last_minutes(minutes)\n",
        "\n",
        "    filt = f\"prefix {'more ' if more_specifics else ''}{prefix}\"\n",
        "    kwargs = dict(\n",
        "        from_time=start_ts,\n",
        "        until_time=end_ts,\n",
        "        record_type=record_type,\n",
        "        filter=filt,\n",
        "    )\n",
        "    if projects is None:\n",
        "        projects = [\"ris\", \"routeviews\"]\n",
        "    if projects:\n",
        "        kwargs[\"projects\"] = projects\n",
        "    if collectors:\n",
        "        kwargs[\"collectors\"] = collectors\n",
        "\n",
        "    stream = pybgpstream.BGPStream(**kwargs)\n",
        "\n",
        "    print(f\"Time window: {_fmt_ts(start_ts)} → {_fmt_ts(end_ts)} (UTC)\")\n",
        "    print(\"Project/Collector           PeerASN  Type  ElemTime                Prefix               AS_PATH\")\n",
        "    print(\"-\" * 110)\n",
        "\n",
        "    count = 0\n",
        "    for rec in stream.records():\n",
        "        if getattr(rec, \"status\", None) not in (None, \"valid\"):\n",
        "            continue\n",
        "        for elem in rec:\n",
        "            etype = elem.type  # 'A', 'W', 'R'\n",
        "            if etype == \"W\" and not include_withdrawals:\n",
        "                continue\n",
        "            if etype not in (\"A\", \"R\", \"W\"):\n",
        "                continue\n",
        "\n",
        "            f = elem.fields\n",
        "            pfx = (\n",
        "                f.get(\"prefix\")\n",
        "                or f.get(\"announced-prefix\")\n",
        "                or f.get(\"withdrawn-prefix\")\n",
        "                or \"?\"\n",
        "            )\n",
        "            as_path = f.get(\"as-path\", \"\")\n",
        "\n",
        "            print(\n",
        "                f\"{rec.project}/{rec.collector:<24} \"\n",
        "                f\"{(elem.peer_asn if elem.peer_asn is not None else '?'):>7}  \"\n",
        "                f\"{etype:^4}  \"\n",
        "                f\"{_fmt_ts(elem.time)}  \"\n",
        "                f\"{pfx:<19}  \"\n",
        "                f\"{as_path}\"\n",
        "            )\n",
        "            count += 1\n",
        "\n",
        "    if count == 0:\n",
        "        print(\"No matching elements observed in the selected window.\")\n",
        "    return count\n",
        "\n",
        "\n",
        "# ------------ Smoke test helpers ------------\n",
        "NOISY_PREFIXES = [\n",
        "    \"1.1.1.0/24\",   # Cloudflare\n",
        "    \"8.8.8.0/24\",   # Google\n",
        "    \"9.9.9.0/24\",   # Quad9\n",
        "    \"4.2.2.0/24\",   # Level3 legacy anycast\n",
        "]"
      ],
      "metadata": {
        "id": "NmJpol9z7K1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def smoke_test(minutes: int = 15) -> None:\n",
        "    \"\"\"Quickly verify pipeline & connectivity by printing a few elements from noisy prefixes.\"\"\"\n",
        "    # Try RIS first\n",
        "    printed = 0\n",
        "    for pfx in NOISY_PREFIXES:\n",
        "        printed += print_elements(\n",
        "            pfx,\n",
        "            minutes=minutes,\n",
        "            projects=[\"ris\"],\n",
        "            collectors=RIS_COLLECTORS,\n",
        "            include_withdrawals=True,\n",
        "            record_type=\"updates\",\n",
        "        )\n",
        "\n",
        "    # If nothing from RIS, try RouteViews\n",
        "    if printed == 0:\n",
        "        for pfx in NOISY_PREFIXES:\n",
        "            printed += print_elements(\n",
        "                pfx,\n",
        "                minutes=minutes,\n",
        "                projects=[\"routeviews\"],\n",
        "                collectors=RV_COLLECTORS,\n",
        "                include_withdrawals=True,\n",
        "                record_type=\"updates\",\n",
        "            )\n",
        "\n",
        "    if printed == 0:\n",
        "        print(\"\\nStill nothing — likely causes: very short window, network egress blocked, or broker lag.\\n\"\n",
        "              \"Try minutes=60 and ensure outbound HTTPS is allowed from Colab.\")"
      ],
      "metadata": {
        "id": "jGnamuspAE4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smoke_test(minutes=60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0M7DNfYV7V_U",
        "outputId": "fc2444d5-3dfe-46f3-d896-8fa763d9ec3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time window: 2025-08-15 13:46:05Z → 2025-08-15 14:46:05Z (UTC)\n",
            "Project/Collector           PeerASN  Type  ElemTime                Prefix               AS_PATH\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "ris/rrc15                     263237   A    2025-08-15 13:52:37Z  1.1.1.0/24           263237 265409 13335\n",
            "ris/rrc15                     263237   A    2025-08-15 13:53:07Z  1.1.1.0/24           263237 174 13335\n",
            "ris/rrc25                     328840   A    2025-08-15 14:01:17Z  1.1.1.0/24           328840 37282 13335\n",
            "ris/rrc15                     263237   A    2025-08-15 14:02:39Z  1.1.1.0/24           263237 265409 13335\n",
            "ris/rrc15                     263237   A    2025-08-15 14:03:07Z  1.1.1.0/24           263237 174 13335\n",
            "ris/rrc15                     263237   A    2025-08-15 14:05:27Z  1.1.1.0/24           263237 262557 13335\n",
            "ris/rrc15                     263237   A    2025-08-15 14:05:52Z  1.1.1.0/24           263237 174 13335\n",
            "ris/rrc03                     212483   A    2025-08-15 14:08:05Z  1.1.1.0/24           212483 13335\n",
            "ris/rrc03                     212483   A    2025-08-15 14:08:05Z  1.1.1.0/24           212483 13335\n",
            "ris/rrc03                     212483   A    2025-08-15 14:09:32Z  1.1.1.0/24           212483 13335\n",
            "ris/rrc03                     212483   A    2025-08-15 14:09:32Z  1.1.1.0/24           212483 13335\n",
            "ris/rrc03                     212635   W    2025-08-15 14:12:21Z  1.1.1.0/24           \n",
            "ris/rrc15                     263237   A    2025-08-15 14:12:39Z  1.1.1.0/24           263237 265409 13335\n",
            "ris/rrc15                     263237   A    2025-08-15 14:13:07Z  1.1.1.0/24           263237 174 13335\n",
            "ris/rrc15                     263237   A    2025-08-15 14:15:28Z  1.1.1.0/24           263237 262557 13335\n",
            "ris/rrc15                     263237   A    2025-08-15 14:15:57Z  1.1.1.0/24           263237 174 13335\n",
            "ris/rrc15                     263237   A    2025-08-15 14:22:41Z  1.1.1.0/24           263237 265409 13335\n",
            "ris/rrc15                     263237   A    2025-08-15 14:23:07Z  1.1.1.0/24           263237 174 13335\n",
            "ris/rrc15                     263237   A    2025-08-15 14:25:29Z  1.1.1.0/24           263237 262557 13335\n",
            "ris/rrc15                     263237   A    2025-08-15 14:25:57Z  1.1.1.0/24           263237 174 13335\n",
            "ris/rrc15                     263237   A    2025-08-15 14:32:41Z  1.1.1.0/24           263237 265409 13335\n",
            "ris/rrc15                     263237   A    2025-08-15 14:33:07Z  1.1.1.0/24           263237 174 13335\n",
            "ris/rrc15                     263237   A    2025-08-15 14:35:30Z  1.1.1.0/24           263237 262557 13335\n",
            "ris/rrc15                     263237   A    2025-08-15 14:35:58Z  1.1.1.0/24           263237 174 13335\n",
            "ris/rrc15                     263237   A    2025-08-15 14:42:43Z  1.1.1.0/24           263237 265409 13335\n",
            "ris/rrc15                     263237   A    2025-08-15 14:43:12Z  1.1.1.0/24           263237 174 13335\n",
            "Time window: 2025-08-15 13:48:40Z → 2025-08-15 14:48:40Z (UTC)\n",
            "Project/Collector           PeerASN  Type  ElemTime                Prefix               AS_PATH\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "ris/rrc25                     328840   A    2025-08-15 14:01:16Z  8.8.8.0/24           328840 60171 15169\n",
            "ris/rrc19                     328320   A    2025-08-15 14:09:03Z  8.8.8.0/24           328320 15169\n",
            "ris/rrc03                     212635   W    2025-08-15 14:13:15Z  8.8.8.0/24           \n",
            "ris/rrc19                     328320   A    2025-08-15 14:23:19Z  8.8.8.0/24           328320 15169\n",
            "ris/rrc19                     328320   W    2025-08-15 14:27:42Z  8.8.8.0/24           \n",
            "ris/rrc19                     328320   A    2025-08-15 14:40:41Z  8.8.8.0/24           328320 15169\n",
            "Time window: 2025-08-15 13:50:51Z → 2025-08-15 14:50:51Z (UTC)\n",
            "Project/Collector           PeerASN  Type  ElemTime                Prefix               AS_PATH\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "ris/rrc03                     212635   W    2025-08-15 14:13:19Z  9.9.9.0/24           \n",
            "ris/rrc25                       1257   A    2025-08-15 14:42:58Z  9.9.9.0/24           1257 42 19281\n",
            "Time window: 2025-08-15 13:53:25Z → 2025-08-15 14:53:25Z (UTC)\n",
            "Project/Collector           PeerASN  Type  ElemTime                Prefix               AS_PATH\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "No matching elements observed in the selected window.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def moas_conflicts(\n",
        "    prefix: str,\n",
        "    window_min: int = 10,\n",
        "    *,\n",
        "    projects: Optional[List[str]] = None,\n",
        "    collectors: Optional[List[str]] = None,\n",
        "    record_type: str = \"updates\",\n",
        "    more_specifics: bool = True,\n",
        "):\n",
        "    \"\"\"Return MOAS info for *prefix* family by *union across vantages*.\n",
        "\n",
        "    Returns: (moas_map, per_prefix_vantages, (start_ts, end_ts), index)\n",
        "      - moas_map: {prefix -> set(origin ASNs)} where len(set) > 1\n",
        "      - per_prefix_vantages: {prefix -> {(project,collector,peer_asn) -> set(origins)}}\n",
        "      - (start_ts, end_ts): time window used\n",
        "      - index: the full vantage index from your pipeline (for deeper inspection)\n",
        "    \"\"\"\n",
        "    conflicts, index, (start_ts, end_ts), _fam = conflicting_origins_by_vantage(\n",
        "        prefix=prefix,\n",
        "        window_min=window_min,\n",
        "        projects=projects,\n",
        "        collectors=collectors,\n",
        "        record_type=record_type,\n",
        "        more_specifics=more_specifics,\n",
        "    )\n",
        "\n",
        "    # Union origins across all vantages for each prefix\n",
        "    per_prefix_union: Dict[str, Set[int]] = defaultdict(set)\n",
        "    per_prefix_vantages: Dict[str, Dict[Tuple[str,str,Optional[int]], Set[int]]] = defaultdict(lambda: defaultdict(set))\n",
        "\n",
        "    for vantage, pmap in index.items():  # vantage = (project, collector, peer_asn)\n",
        "        for pfx, omap in pmap.items():   # omap: {origin_asn -> meta}\n",
        "            origins = set(omap.keys())\n",
        "            if not origins:\n",
        "                continue\n",
        "            per_prefix_union[pfx].update(origins)\n",
        "            per_prefix_vantages[pfx][vantage].update(origins)\n",
        "\n",
        "    # Keep only prefixes with >1 distinct origin → MOAS\n",
        "    moas_map = {pfx: origins for pfx, origins in per_prefix_union.items() if len(origins) > 1}\n",
        "    return moas_map, per_prefix_vantages, (start_ts, end_ts), index"
      ],
      "metadata": {
        "id": "wQHqKtXoJn8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_moas(\n",
        "    moas_map: Dict[str, Set[int]],\n",
        "    per_prefix_vantages: Dict[str, Dict[Tuple[str,str,Optional[int]], Set[int]]],\n",
        "    index, *,\n",
        "    start_ts: int,\n",
        "    end_ts: int,\n",
        ") -> None:\n",
        "    print(f\"Time window: {ts_str(start_ts)} → {ts_str(end_ts)} (UTC)\")\n",
        "    if not moas_map:\n",
        "        print(\"No MOAS observed for the selected prefix family and window.\")\n",
        "        return\n",
        "\n",
        "    for pfx in sorted(moas_map.keys()):\n",
        "        all_orig = \", \".join(f\"AS{o}\" for o in sorted(moas_map[pfx]))\n",
        "        print(f\"\\n{pfx} — MOAS origins: {all_orig}\")\n",
        "        # Show vantage breakdown and per‑origin timing from the existing index\n",
        "        vmap = per_prefix_vantages.get(pfx, {})\n",
        "        for (project, collector, peer_asn) in sorted(vmap.keys()):\n",
        "            origins_here = \", \".join(f\"AS{o}\" for o in sorted(vmap[(project, collector, peer_asn)]))\n",
        "            print(f\"  {project}/{collector} peer_asn={peer_asn} → {origins_here}\")\n",
        "            # Optional: first/last/count per origin using the index meta we already built\n",
        "            omap = index[(project, collector, peer_asn)][pfx]\n",
        "            for o in sorted(omap.keys()):\n",
        "                meta = omap[o]\n",
        "                print(\n",
        "                    f\"    {o}: first={ts_str(meta['first'])}, last={ts_str(meta['last'])}, count={meta['count']}\"\n",
        "                )"
      ],
      "metadata": {
        "id": "QNGwOegNJqBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_moas_table(moas_map: Dict[str, Set[int]], per_prefix_vantages):\n",
        "    if pd is None:\n",
        "        return None\n",
        "    rows = []\n",
        "    for pfx, origins in moas_map.items():\n",
        "        vantages = list(per_prefix_vantages.get(pfx, {}).keys())\n",
        "        rows.append(dict(\n",
        "            prefix=pfx,\n",
        "            n_origins=len(origins),\n",
        "            origins=sorted(origins),\n",
        "            n_vantages=len(vantages),\n",
        "            vantages=[f\"{p}/{c}:{a}\" for (p,c,a) in vantages],\n",
        "        ))\n",
        "    return pd.DataFrame(rows).sort_values([\"n_origins\",\"prefix\"], ascending=[False, True]) if rows else pd.DataFrame(columns=[\"prefix\",\"n_origins\",\"origins\",\"n_vantages\",\"vantages\"])  # type: ignore\n"
      ],
      "metadata": {
        "id": "nwqn7Fd-JsfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import time\n",
        "import datetime as dt\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from typing import Dict, Iterable, Iterator, List, Optional, Set, Tuple\n",
        "import pybgpstream\n",
        "import pandas as pd\n",
        "from itertools import islice\n",
        "\n",
        "\n",
        "def conflicting_origins_by_vantage(prefix: str,\n",
        "                   window_min: int = 10,\n",
        "                   *,\n",
        "                   projects: Optional[List[str]] = None,\n",
        "                   collectors: Optional[List[str]] = None,\n",
        "                   record_type: str = \"updates\",\n",
        "                   more_specifics: bool = True):\n",
        "    \"\"\"Compatibility passthrough in case your namespace differs.\n",
        "    Uses your existing conflicting_origins_by_vantage() then unions per prefix.\n",
        "    \"\"\"\n",
        "    conflicts, index, (start_ts, end_ts), _fam = conflicting_origins_by_vantage(\n",
        "        prefix=prefix,\n",
        "        window_min=window_min,\n",
        "        projects=projects,\n",
        "        collectors=collectors,\n",
        "        record_type=record_type,\n",
        "        more_specifics=more_specifics,\n",
        "    )\n",
        "    per_prefix_union: Dict[str, Set[int]] = defaultdict(set)\n",
        "    for v, pmap in index.items():\n",
        "        for pfx, omap in pmap.items():\n",
        "            per_prefix_union[pfx].update(omap.keys())\n",
        "    moas_map = {pfx: s for pfx, s in per_prefix_union.items() if len(s) > 1}\n",
        "    return moas_map, index, (start_ts, end_ts)\n",
        "\n",
        "\n",
        "def moas_conflicts_ribs_then_updates(\n",
        "    prefix: str,\n",
        "    *,\n",
        "    rib_window_min: int = 360,   # 6h — likely to include a RouteViews/RIS RIB snapshot\n",
        "    upd_window_min: int = 60,    # 1h updates fallback\n",
        "    projects: Optional[List[str]] = None,\n",
        "    collectors: Optional[List[str]] = None,\n",
        "    more_specifics: bool = True,\n",
        "):\n",
        "    \"\"\"Find MOAS for a family by trying RIBs (wider window) then Updates.\n",
        "\n",
        "    Returns: (moas_map, index, (start_ts, end_ts), source)\n",
        "      - source ∈ {\"ribs\", \"updates\"} indicates where MOAS was found (if any).\n",
        "    \"\"\"\n",
        "    # Phase A: RIBs with a wide enough window\n",
        "    moas_map, index, (s1, e1) = moas_conflicts(\n",
        "        prefix=prefix,\n",
        "        window_min=rib_window_min,\n",
        "        projects=projects,\n",
        "        collectors=collectors,\n",
        "        record_type=\"ribs\",\n",
        "        more_specifics=more_specifics,\n",
        "    )\n",
        "    if moas_map:\n",
        "        return moas_map, index, (s1, e1), \"ribs\"\n",
        "\n",
        "    # Phase B: Updates in a moderate window\n",
        "    moas_map, index, (s2, e2) = moas_conflicts(\n",
        "        prefix=prefix,\n",
        "        window_min=upd_window_min,\n",
        "        projects=projects,\n",
        "        collectors=collectors,\n",
        "        record_type=\"updates\",\n",
        "        more_specifics=more_specifics,\n",
        "    )\n",
        "    return moas_map, index, (s2, e2), \"updates\"\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 2) Any‑prefix MOAS discovery with limits (0.0.0.0/0 and ::/0)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def _limited(it: Iterable[dict], max_elems: Optional[int]) -> Iterable[dict]:\n",
        "    if max_elems is None:\n",
        "        return it\n",
        "    return islice(it, max_elems)\n",
        "\n",
        "\n",
        "def find_moas_anyprefix(\n",
        "    minutes: int = 15,\n",
        "    *,\n",
        "    projects: Optional[List[str]] = None,\n",
        "    collectors: Optional[List[str]] = None,\n",
        "    ip_versions: Set[str] = {\"v4\"},      # {\"v4\"}, {\"v6\"}, or {\"v4\",\"v6\"}\n",
        "    record_type: str = \"updates\",\n",
        "    more_specifics: bool = True,\n",
        "    max_elems: Optional[int] = 20000,     # guardrail for notebooks\n",
        "):\n",
        "    \"\"\"Scan a short window for MOAS anywhere (not just a given family).\n",
        "\n",
        "    Strategy:\n",
        "      - Build a vantage→prefix→origins index from /0 families (IPv4/IPv6) with a\n",
        "        safe cap on number of elements, then union per prefix.\n",
        "      - Reuses your existing: make_stream → iter_bgp_elements → build_vantage_origin_index.\n",
        "\n",
        "    Returns: (moas_map, index, (start_ts, end_ts)) for the scanned set.\n",
        "    \"\"\"\n",
        "    from_time, until_time = window_last_minutes(minutes)\n",
        "\n",
        "    def index_family(family_prefix: str):\n",
        "        stream = make_stream(\n",
        "            family_prefix,\n",
        "            from_time,\n",
        "            until_time,\n",
        "            projects=projects,\n",
        "            collectors=collectors,\n",
        "            record_type=record_type,\n",
        "            more_specifics=more_specifics,\n",
        "        )\n",
        "        elems = iter_bgp_elements(stream)\n",
        "        elems = _limited(elems, max_elems)\n",
        "        return build_vantage_origin_index(elems)\n",
        "\n",
        "    combined_index: Dict[Tuple[str,str,Optional[int]], Dict[str, Dict[int, Dict[str,int]]]] = defaultdict(dict)\n",
        "\n",
        "    families = []\n",
        "    if \"v4\" in ip_versions:\n",
        "        families.append(\"0.0.0.0/0\")\n",
        "    if \"v6\" in ip_versions:\n",
        "        families.append(\"::/0\")\n",
        "\n",
        "    for fam in families:\n",
        "        idx = index_family(fam)\n",
        "        # shallow merge\n",
        "        for v, pmap in idx.items():\n",
        "            combined_index.setdefault(v, {})\n",
        "            for pfx, omap in pmap.items():\n",
        "                combined_index[v].setdefault(pfx, {})\n",
        "                combined_index[v][pfx].update(omap)\n",
        "\n",
        "    # Union origins per prefix across all vantages\n",
        "    per_prefix_union: Dict[str, Set[int]] = defaultdict(set)\n",
        "    for pmap in combined_index.values():\n",
        "        for pfx, omap in pmap.items():\n",
        "            per_prefix_union[pfx].update(omap.keys())\n",
        "\n",
        "    moas_map = {pfx: s for pfx, s in per_prefix_union.items() if len(s) > 1}\n",
        "    return moas_map, combined_index, (from_time, until_time)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 3) Pretty printers (optional)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def print_moas_map(moas_map, index, *, start_ts: int, end_ts: int, max_prefixes: int = 20):\n",
        "    print(f\"Time window: {ts_str(start_ts)} → {ts_str(end_ts)} (UTC)\")\n",
        "    if not moas_map:\n",
        "        print(\"No MOAS observed in this scan.\")\n",
        "        return\n",
        "    shown = 0\n",
        "    for pfx in sorted(moas_map.keys()):\n",
        "        if shown >= max_prefixes:\n",
        "            print(f\"... (truncated after {max_prefixes} prefixes)\")\n",
        "            break\n",
        "        origins = \", \".join(f\"AS{o}\" for o in sorted(moas_map[pfx]))\n",
        "        print(f\"\\n{pfx} — MOAS origins: {origins}\")\n",
        "        # show a couple of vantages where seen\n",
        "        for v, pmap in index.items():\n",
        "            if pfx in pmap:\n",
        "                vs = \", \".join(f\"AS{o}\" for o in sorted(pmap[pfx].keys()))\n",
        "                print(f\"  {v[0]}/{v[1]} peer_asn={v[2]} → {vs}\")\n",
        "        shown += 1"
      ],
      "metadata": {
        "id": "moKB-IQ91Srf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A) Try to find MOAS for a specific family using RIBs first\n",
        "moas_map, idx, (s, e), src = moas_conflicts_ribs_then_updates(\n",
        "    prefix=\"1.1.1.0/24\",\n",
        "    rib_window_min=360,\n",
        "    upd_window_min=60,\n",
        "    projects=[\"ris\",\"routeviews\"],\n",
        "    collectors=(RIS_COLLECTORS + RV_COLLECTORS) if 'RIS_COLLECTORS' in globals() else None,\n",
        "    more_specifics=True,\n",
        ")\n",
        "print(f\"\\nSource used: {src}\")\n",
        "print_moas_map(moas_map, idx, start_ts=s, end_ts=e)"
      ],
      "metadata": {
        "id": "fO-ZkRYNNlZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# B) Discover MOAS anywhere (guarded by max_elems)\n",
        "moas_any, idx_any, (s2, e2) = find_moas_anyprefix(\n",
        "    minutes=60,\n",
        "    projects=[\"ris\"],\n",
        "    collectors=RIS_COLLECTORS[:4] if 'RIS_COLLECTORS' in globals() else None,\n",
        "    ip_versions={\"v4\"},\n",
        "    record_type=\"updates\",\n",
        "    more_specifics=True,\n",
        "    max_elems=20000,\n",
        ")\n",
        "print(\"\\n[ANY‑PREFIX scan]\")\n",
        "print_moas_map(moas_any, idx_any, start_ts=s2, end_ts=e2, max_prefixes=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Thc4s5HRaNme",
        "outputId": "b54b19a7-5de8-4991-fc18-ac80b02ad947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[ANY‑PREFIX scan]\n",
            "Time window: 2025-08-15 14:52:55Z → 2025-08-15 15:52:55Z (UTC)\n",
            "\n",
            "177.46.35.0/24 — MOAS origins: AS28135, AS274819\n",
            "  ris/rrc00 peer_asn=34549 → AS274819\n",
            "  ris/rrc00 peer_asn=58057 → AS274819\n",
            "  ris/rrc00 peer_asn=206499 → AS274819\n",
            "  ris/rrc01 peer_asn=6908 → AS274819\n",
            "  ris/rrc01 peer_asn=2914 → AS274819\n",
            "  ris/rrc03 peer_asn=47147 → AS274819\n",
            "  ris/rrc03 peer_asn=48185 → AS274819\n",
            "  ris/rrc01 peer_asn=15692 → AS274819\n",
            "  ris/rrc00 peer_asn=852 → AS274819\n",
            "  ris/rrc00 peer_asn=15562 → AS28135, AS274819\n",
            "  ris/rrc00 peer_asn=4608 → AS274819\n",
            "\n",
            "181.233.80.0/22 — MOAS origins: AS268314, AS271482\n",
            "  ris/rrc00 peer_asn=204092 → AS268314\n",
            "  ris/rrc00 peer_asn=34549 → AS268314, AS271482\n",
            "  ris/rrc00 peer_asn=49432 → AS271482\n",
            "  ris/rrc00 peer_asn=24482 → AS268314, AS271482\n",
            "  ris/rrc00 peer_asn=7018 → AS268314\n",
            "  ris/rrc00 peer_asn=216285 → AS271482\n",
            "  ris/rrc00 peer_asn=37721 → AS268314, AS271482\n",
            "  ris/rrc00 peer_asn=48185 → AS271482\n",
            "  ris/rrc00 peer_asn=29504 → AS268314\n",
            "  ris/rrc00 peer_asn=34854 → AS268314, AS271482\n",
            "  ris/rrc00 peer_asn=58057 → AS268314, AS271482\n",
            "  ris/rrc00 peer_asn=59919 → AS268314\n",
            "  ris/rrc00 peer_asn=206499 → AS268314, AS271482\n",
            "  ris/rrc00 peer_asn=1403 → AS268314\n",
            "  ris/rrc00 peer_asn=14907 → AS268314\n",
            "  ris/rrc01 peer_asn=8218 → AS271482\n",
            "  ris/rrc01 peer_asn=6908 → AS268314, AS271482\n",
            "  ris/rrc01 peer_asn=6830 → AS268314\n",
            "  ris/rrc01 peer_asn=24482 → AS268314, AS271482\n",
            "  ris/rrc01 peer_asn=48185 → AS271482\n",
            "  ris/rrc01 peer_asn=1299 → AS271482\n",
            "  ris/rrc01 peer_asn=917 → AS268314, AS271482\n",
            "  ris/rrc01 peer_asn=13030 → AS271482\n",
            "  ris/rrc01 peer_asn=39122 → AS271482\n",
            "  ris/rrc01 peer_asn=36924 → AS268314\n",
            "  ris/rrc01 peer_asn=62167 → AS268314\n",
            "  ris/rrc01 peer_asn=2914 → AS271482\n",
            "  ris/rrc01 peer_asn=42473 → AS268314\n",
            "  ris/rrc01 peer_asn=3257 → AS268314, AS271482\n",
            "  ris/rrc01 peer_asn=6233 → AS271482\n",
            "  ris/rrc04 peer_asn=15547 → AS271482\n",
            "  ris/rrc04 peer_asn=513 → AS268314\n",
            "  ris/rrc04 peer_asn=29222 → AS268314\n",
            "  ris/rrc03 peer_asn=6830 → AS268314\n",
            "  ris/rrc03 peer_asn=6233 → AS271482\n",
            "  ris/rrc03 peer_asn=47147 → AS268314\n",
            "  ris/rrc03 peer_asn=39351 → AS271482\n",
            "  ris/rrc03 peer_asn=212483 → AS271482\n",
            "  ris/rrc03 peer_asn=42541 → AS271482\n",
            "  ris/rrc03 peer_asn=14907 → AS268314\n",
            "  ris/rrc03 peer_asn=8455 → AS268314, AS271482\n",
            "  ris/rrc03 peer_asn=48185 → AS271482\n",
            "  ris/rrc03 peer_asn=1103 → AS271482\n",
            "  ris/rrc03 peer_asn=12859 → AS271482\n",
            "  ris/rrc03 peer_asn=24482 → AS271482\n",
            "  ris/rrc03 peer_asn=205206 → AS271482\n",
            "  ris/rrc03 peer_asn=199938 → AS271482\n",
            "  ris/rrc03 peer_asn=209650 → AS271482\n",
            "  ris/rrc03 peer_asn=1140 → AS271482\n",
            "  ris/rrc03 peer_asn=49544 → AS268314\n",
            "  ris/rrc03 peer_asn=8218 → AS271482\n",
            "  ris/rrc03 peer_asn=50763 → AS268314\n",
            "  ris/rrc03 peer_asn=35598 → AS268314\n",
            "  ris/rrc03 peer_asn=37721 → AS271482\n",
            "  ris/rrc01 peer_asn=15692 → AS271482\n",
            "  ris/rrc01 peer_asn=9002 → AS268314\n",
            "  ris/rrc01 peer_asn=3170 → AS268314, AS271482\n",
            "  ris/rrc00 peer_asn=202365 → AS268314\n",
            "  ris/rrc00 peer_asn=44393 → AS268314, AS271482\n",
            "  ris/rrc00 peer_asn=852 → AS268314, AS271482\n",
            "  ris/rrc00 peer_asn=57821 → AS268314, AS271482\n",
            "  ris/rrc00 peer_asn=22652 → AS268314, AS271482\n",
            "  ris/rrc00 peer_asn=34800 → AS268314\n",
            "  ris/rrc00 peer_asn=34927 → AS268314, AS271482\n",
            "  ris/rrc00 peer_asn=15562 → AS268314, AS271482\n",
            "  ris/rrc04 peer_asn=12350 → AS271482\n",
            "  ris/rrc03 peer_asn=8529 → AS268314\n",
            "  ris/rrc00 peer_asn=45049 → AS268314\n",
            "  ris/rrc01 peer_asn=48070 → AS271482\n",
            "  ris/rrc03 peer_asn=12956 → AS268314\n",
            "  ris/rrc00 peer_asn=57381 → AS271482\n",
            "  ris/rrc00 peer_asn=50628 → AS268314\n",
            "  ris/rrc00 peer_asn=38001 → AS268314\n",
            "  ris/rrc00 peer_asn=50304 → AS271482\n",
            "  ris/rrc00 peer_asn=132825 → AS271482\n",
            "\n",
            "185.39.51.0/24 — MOAS origins: AS142111, AS152672\n",
            "  ris/rrc04 peer_asn=15547 → AS142111, AS152672\n",
            "\n",
            "62.72.171.0/24 — MOAS origins: AS142111, AS152672\n",
            "  ris/rrc04 peer_asn=15547 → AS142111, AS152672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BGPStream Graph Analysis**"
      ],
      "metadata": {
        "id": "HYgv1JDNQ9Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"tools/bgpstream/as_graph.pickle\", \"rb\") as f:\n",
        "    G = pickle.load(f)"
      ],
      "metadata": {
        "id": "3MVw1bO8D0T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "asn = '1299'\n",
        "total_degree = G.degree(asn)\n",
        "print(total_degree)"
      ],
      "metadata": {
        "id": "eNh8YnG8D6GA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffb82274-bd3a-4032-b21a-93deb56cb544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AS Graph Tools**"
      ],
      "metadata": {
        "id": "uPMWAloCIuw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _largest_component_subgraph(G: nx.Graph) -> nx.Graph:\n",
        "    \"\"\"Return the largest weakly‑connected component as an *undirected* graph.\"\"\"\n",
        "    if G.is_directed():\n",
        "        comp_nodes = max(nx.weakly_connected_components(G), key=len)\n",
        "    else:\n",
        "        comp_nodes = max(nx.connected_components(G), key=len)\n",
        "    return G.subgraph(comp_nodes).to_undirected()\n",
        "\n",
        "\n",
        "def _percentile(value: float, sample: Sequence[float]) -> float:\n",
        "    \"\"\"Return the *inclusive* percentile (0‑100) of *value* within *sample*.\"\"\"\n",
        "    count = sum(1 for v in sample if v <= value)\n",
        "    return 100 * count / len(sample)"
      ],
      "metadata": {
        "id": "jVip1t1DIxuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def as_degree(G: nx.DiGraph, asn: str | int, *, normalized: bool = True) -> Dict[str, Any]:\n",
        "    \"\"\"Return in‑, out‑ and total degree of **asn**.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    G : nx.DiGraph or nx.Graph\n",
        "        AS‑relation graph (directed preferred).\n",
        "    asn : str | int\n",
        "        Autonomous‑System Number (must exist in *G*).\n",
        "    normalized : bool, default **True**\n",
        "        If *True* include percentile among all nodes.\n",
        "    \"\"\"\n",
        "    if asn not in G:\n",
        "        raise KeyError(f\"ASN {asn} not found in graph\")\n",
        "\n",
        "    indeg = G.in_degree(asn) if G.is_directed() else G.degree(asn)\n",
        "    outdeg = G.out_degree(asn) if G.is_directed() else G.degree(asn)\n",
        "    total = indeg + outdeg if G.is_directed() else G.degree(asn)\n",
        "\n",
        "    result: Dict[str, Any] = {\"in\": indeg, \"out\": outdeg, \"total\": total}\n",
        "\n",
        "    if normalized:\n",
        "        totals = [G.in_degree(n) + G.out_degree(n) if G.is_directed() else G.degree(n) for n in G]\n",
        "        result[\"percentile\"] = round(_percentile(total, totals), 2)\n",
        "    return result"
      ],
      "metadata": {
        "id": "sLdW5aWbIycC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def as_betweenness(\n",
        "    G: nx.DiGraph,\n",
        "    asn: str | int,\n",
        "    *,\n",
        "    k: Optional[int] = 2000,\n",
        "    normalized: bool = True,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Betweenness‑centrality for a single node.\n",
        "\n",
        "    *k* – number of samples for *approximate* algorithm.  Set to *None* to run\n",
        "    exact computation (be careful: O(V·E)).\n",
        "    \"\"\"\n",
        "    if asn not in G:\n",
        "        raise KeyError(asn)\n",
        "\n",
        "    if k is None:\n",
        "        bet = nx.betweenness_centrality(G, normalized=normalized)\n",
        "        score = bet[asn]\n",
        "    else:\n",
        "        score = nx.betweenness_centrality_subset(\n",
        "            G,\n",
        "            sources=random.sample(list(G.nodes()), min(k, len(G))),\n",
        "            targets=random.sample(list(G.nodes()), min(k, len(G))),\n",
        "            normalized=normalized,\n",
        "        )[asn]\n",
        "    # Rank – how many nodes have strictly higher score\n",
        "    higher = sum(1 for v in (nx.betweenness_centrality(G, k=k).values()) if v > score)\n",
        "    return {\"score\": score, \"rank\": higher + 1}"
      ],
      "metadata": {
        "id": "N8vx0o58I1DS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def as_closeness(G: nx.DiGraph, asn: str | int) -> float:\n",
        "    \"\"\"Return closeness‑centrality (harmonic) of **asn**.\"\"\"\n",
        "    return nx.closeness_centrality(G, asn)"
      ],
      "metadata": {
        "id": "eyButmdZI1FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def as_eigenvector(\n",
        "    G: nx.DiGraph,\n",
        "    asn: str | int,\n",
        "    *,\n",
        "    variant: str = \"pagerank\",\n",
        "    alpha: float = 0.85,\n",
        ") -> float:\n",
        "    \"\"\"Return eigenvector‑based centrality.\n",
        "\n",
        "    *variant* ∈ {'pagerank', 'eigenvector', 'katz'}\n",
        "    \"\"\"\n",
        "    if variant == \"pagerank\":\n",
        "        pr = nx.pagerank(G, alpha=alpha)\n",
        "        return pr[asn]\n",
        "    elif variant == \"eigenvector\":\n",
        "        ev = nx.eigenvector_centrality(G, max_iter=500)\n",
        "        return ev[asn]\n",
        "    elif variant == \"katz\":\n",
        "        katz = nx.katz_centrality_numpy(G, alpha=0.005)\n",
        "        return katz[asn]\n",
        "    else:\n",
        "        raise ValueError(\"Unknown variant\")"
      ],
      "metadata": {
        "id": "GmIRMN9OI1HJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def as_kcore_index(G: nx.DiGraph, asn: str | int) -> int:\n",
        "    \"\"\"Return k‑core index (coreness) of **asn**.\"\"\"\n",
        "    core_num = nx.core_number(G.to_undirected())\n",
        "    return core_num[asn]"
      ],
      "metadata": {
        "id": "zfFIcIA5I1Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def as_local_clustering(G: nx.DiGraph, asn: str | int) -> float:\n",
        "    \"\"\"Local clustering coefficient around **asn** (ignores edge direction).\"\"\"\n",
        "    return nx.clustering(G.to_undirected(), asn)"
      ],
      "metadata": {
        "id": "8jyhpg1GI_Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def as_articulation_flag(G: nx.DiGraph, asn: str | int) -> bool:\n",
        "    \"\"\"Return *True* if **asn** is an articulation point (removal disconnects component).\"\"\"\n",
        "    UG = G.to_undirected()\n",
        "    return asn in nx.articulation_points(UG)"
      ],
      "metadata": {
        "id": "IILPMbiwI_Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def as_removal_impact(\n",
        "    G: nx.DiGraph,\n",
        "    asn: str | int,\n",
        "    metrics: Sequence[str] | None = None,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Simulate removal of **asn** and report delta for selected *metrics*.\n",
        "\n",
        "    Supported metrics: 'components', 'diameter', 'avg_shortest_path'.\n",
        "    \"\"\"\n",
        "    metrics = metrics or (\"components\",)\n",
        "    UG = G.to_undirected()\n",
        "    base_comp = nx.number_connected_components(UG)\n",
        "    if \"diameter\" in metrics or \"avg_shortest_path\" in metrics:\n",
        "        base_lcc = _largest_component_subgraph(G)\n",
        "        base_diam = nx.diameter(base_lcc)\n",
        "        base_aspl = nx.average_shortest_path_length(base_lcc)\n",
        "    # Remove\n",
        "    H = G.copy()\n",
        "    H.remove_node(asn)\n",
        "    results: Dict[str, Any] = {}\n",
        "    if \"components\" in metrics:\n",
        "        results[\"components\"] = {\n",
        "            \"before\": base_comp,\n",
        "            \"after\": nx.number_connected_components(H.to_undirected()),\n",
        "        }\n",
        "    if \"diameter\" in metrics or \"avg_shortest_path\" in metrics:\n",
        "        if H.number_of_nodes() == 0:\n",
        "            results[\"diameter\"] = {\"before\": base_diam, \"after\": None}\n",
        "            results[\"avg_shortest_path\"] = {\"before\": base_aspl, \"after\": None}\n",
        "        else:\n",
        "            H_lcc = _largest_component_subgraph(H)\n",
        "            if \"diameter\" in metrics:\n",
        "                results[\"diameter\"] = {\n",
        "                    \"before\": base_diam,\n",
        "                    \"after\": nx.diameter(H_lcc),\n",
        "                }\n",
        "            if \"avg_shortest_path\" in metrics:\n",
        "                results[\"avg_shortest_path\"] = {\n",
        "                    \"before\": base_aspl,\n",
        "                    \"after\": nx.average_shortest_path_length(H_lcc),\n",
        "                }\n",
        "    return results"
      ],
      "metadata": {
        "id": "73ycsynMI_EY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def as_two_hop_neighbors(G: nx.DiGraph, asn: str | int) -> List[str]:\n",
        "    \"\"\"Return list of neighbours within ≤ 2 hops from **asn** (inclusive).\"\"\"\n",
        "    lengths = nx.single_source_shortest_path_length(G.to_undirected(), asn, cutoff=2)\n",
        "    return [n for n, d in lengths.items() if 1 <= d <= 2]"
      ],
      "metadata": {
        "id": "I4Ff8lHOJKhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def as_role_classifier(G: nx.DiGraph, asn: str | int) -> str:\n",
        "    \"\"\"Heuristic role classification: Tier‑1 | Tier‑2 | Stub | Content | Enterprise.\"\"\"\n",
        "    deg = as_degree(G, asn, normalized=False)\n",
        "    cone_size = len(as_two_hop_neighbors(G, asn))\n",
        "    core = as_kcore_index(G, asn)\n",
        "\n",
        "    if deg[\"in\"] == 0:  # no upstream – characteristic of Tier‑1\n",
        "        return \"Tier‑1\"\n",
        "    if core > 20 and cone_size > 1000:\n",
        "        return \"Tier‑2\"\n",
        "    if deg[\"total\"] < 5 and cone_size < 50:\n",
        "        return \"Stub\"\n",
        "    if deg[\"out\"] > deg[\"in\"] * 2:\n",
        "        return \"Content/Enterprise\"\n",
        "    return \"Regional ISP\""
      ],
      "metadata": {
        "id": "_08OMpCgJMmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def graph_density(G: nx.DiGraph) -> float:\n",
        "    \"\"\"Return edge density (0‑1) – the fraction of possible edges that exist.\"\"\"\n",
        "    return nx.density(G)"
      ],
      "metadata": {
        "id": "QNxdKIMEJVGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def graph_degree_distribution(G: nx.DiGraph) -> Dict[str, Any]:\n",
        "    \"\"\"Return statistical summary of degree distribution (undirected view).\"\"\"\n",
        "    degs = [G.in_degree(n) + G.out_degree(n) if G.is_directed() else G.degree(n) for n in G]\n",
        "    summary = {\n",
        "        \"min\": min(degs),\n",
        "        \"max\": max(degs),\n",
        "        \"mean\": statistics.fmean(degs),\n",
        "        \"median\": statistics.median(degs),\n",
        "        \"stdev\": statistics.pstdev(degs),\n",
        "        \"gini\": _gini(degs),\n",
        "    }\n",
        "    # Power‑law exponent (alpha) via log‑log linear regression if possible.\n",
        "    if len(degs) > 10:\n",
        "        alpha = _power_law_alpha(degs)\n",
        "        summary[\"power_law_alpha\"] = alpha\n",
        "    return summary"
      ],
      "metadata": {
        "id": "ZR9OUB0eJVX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _gini(values: Sequence[int | float]) -> float:\n",
        "    \"\"\"Gini coefficient (0 perfect equality, 1 max inequality).\"\"\"\n",
        "    sorted_vals = sorted(values)\n",
        "    n = len(values)\n",
        "    cum = 0\n",
        "    for i, v in enumerate(sorted_vals, 1):\n",
        "        cum += v * i\n",
        "    return (2 * cum) / (n * sum(sorted_vals)) - (n + 1) / n"
      ],
      "metadata": {
        "id": "sG3W--p4JVaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _power_law_alpha(values: Sequence[int]) -> float:\n",
        "    \"\"\"Return crude power‑law exponent using linear regression on CCDF.\"\"\"\n",
        "    import math, itertools\n",
        "\n",
        "    sorted_vals = sorted(v for v in values if v > 0)\n",
        "    ranks = list(range(1, len(sorted_vals) + 1))\n",
        "    ccdf = [1 - (r - 1) / len(sorted_vals) for r in ranks]\n",
        "    xs = [math.log(v) for v in sorted_vals]\n",
        "    ys = [math.log(c) for c in ccdf]\n",
        "    # Least‑squares slope\n",
        "    n = len(xs)\n",
        "    x_bar = sum(xs) / n\n",
        "    y_bar = sum(ys) / n\n",
        "    num = sum((x - x_bar) * (y - y_bar) for x, y in zip(xs, ys))\n",
        "    den = sum((x - x_bar) ** 2 for x in xs)\n",
        "    slope = num / den if den else 0.0\n",
        "    return -slope  # α ≈ -slope of log‑log CCDF"
      ],
      "metadata": {
        "id": "7dz9PC5yJVb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def graph_diameter(G: nx.DiGraph) -> Dict[str, Any]:\n",
        "    \"\"\"Return diameter, radius, and average shortest path of the giant component.\"\"\"\n",
        "    LCC = _largest_component_subgraph(G)\n",
        "    return {\n",
        "        \"diameter\": nx.diameter(LCC),\n",
        "        \"radius\": nx.radius(LCC),\n",
        "        \"avg_shortest_path\": nx.average_shortest_path_length(LCC),\n",
        "    }\n",
        "\n",
        "\n",
        "def graph_modularity_communities(G: nx.DiGraph) -> Dict[str, Any]:\n",
        "    \"\"\"Detect communities using greedy modularity and return mapping + modularity.\"\"\"\n",
        "    UG = G.to_undirected()\n",
        "    communities = list(nx.algorithms.community.greedy_modularity_communities(UG))\n",
        "    mapping = {n: i for i, comm in enumerate(communities) for n in comm}\n",
        "    Q = nx.algorithms.community.quality.modularity(UG, communities)\n",
        "    return {\"Q\": Q, \"communities\": mapping}\n",
        "\n",
        "\n",
        "def graph_assortativity(G: nx.DiGraph) -> float:\n",
        "    \"\"\"Degree assortativity coefficient (−1 disassortative, +1 assortative).\"\"\"\n",
        "    UG = G.to_undirected()\n",
        "    return nx.degree_assortativity_coefficient(UG)\n",
        "\n",
        "\n",
        "def graph_transitivity(G: nx.DiGraph) -> float:\n",
        "    \"\"\"Global clustering coefficient (ratio of closed triplets).\"\"\"\n",
        "    return nx.transitivity(G.to_undirected())\n",
        "\n",
        "\n",
        "def graph_kcore_layers(G: nx.DiGraph) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Return per‑layer statistics of the k‑core decomposition.\"\"\"\n",
        "    core = nx.core_number(G.to_undirected())\n",
        "    layer_stats: Dict[int, List[int]] = defaultdict(list)\n",
        "    for node, k in core.items():\n",
        "        layer_stats[k].append(node)\n",
        "    summary = []\n",
        "    for k in sorted(layer_stats):\n",
        "        nodes = layer_stats[k]\n",
        "        summary.append({\n",
        "            \"k\": k,\n",
        "            \"size\": len(nodes),\n",
        "            \"avg_deg\": statistics.fmean(G.degree(n) for n in nodes),\n",
        "        })\n",
        "    return summary\n",
        "\n",
        "\n",
        "def graph_connectivity(G: nx.DiGraph) -> Dict[str, int]:\n",
        "    \"\"\"Return edge and vertex connectivity of the *undirected* graph.\"\"\"\n",
        "    UG = G.to_undirected()\n",
        "    return {\n",
        "        \"edge_connectivity\": nx.edge_connectivity(UG, approximate=True),\n",
        "        \"vertex_connectivity\": nx.node_connectivity(UG),\n",
        "    }\n",
        "\n",
        "\n",
        "def graph_spectral_gap(G: nx.DiGraph) -> Dict[str, float]:\n",
        "    \"\"\"Return λ₂ and (λ₁ − λ₂) of the Laplacian – indicators of robustness.\n",
        "    Requires *numpy*.\n",
        "    \"\"\"\n",
        "    if np is None:\n",
        "        raise ImportError(\"numpy required for spectral gap calculation\")\n",
        "    UG = G.to_undirected()\n",
        "    L = nx.laplacian_matrix(UG).astype(float)\n",
        "    eigs = np.linalg.eigvalsh(L.A)  # type: ignore\n",
        "    eigs.sort()\n",
        "    return {\"lambda2\": eigs[1], \"spectral_gap\": eigs[-1] - eigs[1]}\n",
        "\n",
        "\n",
        "def graph_redundancy_profile(\n",
        "    G: nx.DiGraph,\n",
        "    *,\n",
        "    sample: int = 2000,\n",
        ") -> Dict[int, int]:\n",
        "    \"\"\"Histogram of edge‑disjoint path multiplicity between random node pairs.\"\"\"\n",
        "    UG = G.to_undirected()\n",
        "    nodes = list(UG.nodes())\n",
        "    hist: Counter[int] = Counter()\n",
        "    for _ in range(min(sample, len(nodes) ** 2)):\n",
        "        s, t = random.sample(nodes, 2)\n",
        "        try:\n",
        "            k = nx.edge_connectivity(UG, s, t)\n",
        "            hist[k] += 1\n",
        "        except nx.NetworkXError:\n",
        "            hist[0] += 1  # disconnected\n",
        "    return dict(hist)\n",
        "\n",
        "\n",
        "def graph_time_compare(\n",
        "    G_old: nx.DiGraph,\n",
        "    G_new: nx.DiGraph,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Compare two snapshots and report added/removed nodes and edges.\"\"\"\n",
        "    edges_old = set(G_old.edges())\n",
        "    edges_new = set(G_new.edges())\n",
        "    nodes_old = set(G_old.nodes())\n",
        "    nodes_new = set(G_new.nodes())\n",
        "    return {\n",
        "        \"edges_added\": list(edges_new - edges_old),\n",
        "        \"edges_removed\": list(edges_old - edges_new),\n",
        "        \"nodes_added\": list(nodes_new - nodes_old),\n",
        "        \"nodes_removed\": list(nodes_old - nodes_new),\n",
        "    }\n"
      ],
      "metadata": {
        "id": "dV1QTZh5JVeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _largest_component_subgraph(G: nx.Graph) -> nx.Graph:\n",
        "    \"\"\"Return the Undirected view of the **largest weakly‑connected component**.\"\"\"\n",
        "    if G.is_directed():\n",
        "        comp_nodes = max(nx.weakly_connected_components(G), key=len)\n",
        "    else:\n",
        "        comp_nodes = max(nx.connected_components(G), key=len)\n",
        "    return G.subgraph(comp_nodes).to_undirected()\n",
        "\n",
        "\n",
        "def _percentile(x: float, sample: Sequence[float]) -> float:\n",
        "    \"\"\"Inclusive percentile of *x* within *sample* (0–100).\"\"\"\n",
        "    leq = sum(1 for v in sample if v <= x)\n",
        "    return 100 * leq / len(sample)"
      ],
      "metadata": {
        "id": "opRDPoLGKueZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def global_resilience_dashboard(G: nx.DiGraph, *, sample: int = 2000) -> Dict[str, Any]:\n",
        "    \"\"\"Return a consolidated resilience report with **pure graph metrics**.\n",
        "\n",
        "    Metrics included\n",
        "    ----------------\n",
        "    • *edge_connectivity* / *vertex_connectivity*\n",
        "    • *spectral_gap* ( λ₁ − λ₂ )\n",
        "    • size of most robust k‑core (k_max)\n",
        "    • average edge‑disjoint path count between random node pairs.\n",
        "    \"\"\"\n",
        "    report: Dict[str, Any] = {}\n",
        "\n",
        "    # Connectivity (approximate edge‑connectivity for performance)\n",
        "    UG = G.to_undirected()\n",
        "    report[\"edge_connectivity\"] = nx.edge_connectivity(UG, approximate=True)\n",
        "    report[\"vertex_connectivity\"] = nx.node_connectivity(UG)\n",
        "\n",
        "    # Spectral gap (requires numpy)\n",
        "    if np is not None and G.number_of_nodes() > 2:\n",
        "        L = nx.laplacian_matrix(UG).astype(float)\n",
        "        eigs = np.linalg.eigvalsh(L.A)  # type: ignore[attr-defined]\n",
        "        eigs.sort()\n",
        "        report[\"lambda2\"] = eigs[1]\n",
        "        report[\"spectral_gap\"] = eigs[-1] - eigs[1]\n",
        "    else:\n",
        "        report[\"lambda2\"] = report[\"spectral_gap\"] = None\n",
        "\n",
        "    # k‑core robustness index\n",
        "    core_numbers = nx.core_number(UG)\n",
        "    report[\"k_max\"] = max(core_numbers.values())\n",
        "\n",
        "    # Average edge‑disjoint paths on random sample\n",
        "    nodes = list(UG.nodes())\n",
        "    hist: Counter[int] = Counter()\n",
        "    for _ in range(min(sample, len(nodes) ** 2)):\n",
        "        s, t = random.sample(nodes, 2)\n",
        "        try:\n",
        "            k = nx.edge_connectivity(UG, s, t)\n",
        "            hist[k] += 1\n",
        "        except nx.NetworkXError:\n",
        "            hist[0] += 1\n",
        "    avg_paths = sum(k * c for k, c in hist.items()) / sum(hist.values())\n",
        "    report[\"avg_edge_disjoint_paths\"] = avg_paths\n",
        "\n",
        "    return report"
      ],
      "metadata": {
        "id": "wsZ3of3kKu3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def critical_edge_set(G: nx.DiGraph, *, budget: int = 20) -> List[Tuple[str, str, float]]:\n",
        "    \"\"\"Return *budget* most critical edges ranked by **edge‑betweenness**.\n",
        "    Each tuple → (u, v, betweenness_score).\"\"\"\n",
        "    betw = nx.edge_betweenness_centrality(G)\n",
        "    return sorted(betw.items(), key=lambda x: x[1], reverse=True)[:budget]"
      ],
      "metadata": {
        "id": "SFTnr_a2KzZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hub_landscape_mapper(G: nx.DiGraph) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Classify each AS into *Global*, *Regional*, *Local* hub or *Stub*.\n",
        "\n",
        "    Heuristic based on degree percentile + eigenvector centrality.\n",
        "    \"\"\"\n",
        "    degs = {n: G.in_degree(n) + G.out_degree(n) if G.is_directed() else G.degree(n) for n in G}\n",
        "    percentiles = {n: _percentile(d, degs.values()) for n, d in degs.items()}\n",
        "    eig = nx.eigenvector_centrality(G, max_iter=500)\n",
        "\n",
        "    mapping = []\n",
        "    for n in G:\n",
        "        if percentiles[n] >= 99 and eig[n] > 0.01:\n",
        "            role = \"Global Hub\"\n",
        "        elif percentiles[n] >= 90:\n",
        "            role = \"Regional Hub\"\n",
        "        elif percentiles[n] >= 70:\n",
        "            role = \"Local Hub\"\n",
        "        else:\n",
        "            role = \"Stub/Edge\"\n",
        "        mapping.append({\n",
        "            \"asn\": n,\n",
        "            \"role\": role,\n",
        "            \"degree\": degs[n],\n",
        "            \"eigenvector\": round(eig[n], 6),\n",
        "            \"degree_percentile\": round(percentiles[n], 2),\n",
        "        })\n",
        "    return mapping"
      ],
      "metadata": {
        "id": "bNpa8fnoKzbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _sample_longest_pairs(G: nx.Graph, k: int) -> List[Tuple[str, str]]:\n",
        "    \"\"\"Return *k* node pairs with the largest shortest‑path lengths (sampling).\"\"\"\n",
        "    UG = G.to_undirected()\n",
        "    nodes = list(UG)\n",
        "    pairs: List[Tuple[str, str]] = []\n",
        "    dists: List[int] = []\n",
        "    for _ in range(k * 5):  # oversample for better chance of long paths\n",
        "        a, b = random.sample(nodes, 2)\n",
        "        try:\n",
        "            d = nx.shortest_path_length(UG, a, b)\n",
        "            pairs.append((a, b))\n",
        "            dists.append(d)\n",
        "        except nx.NetworkXNoPath:\n",
        "            continue\n",
        "    top_idx = sorted(range(len(dists)), key=lambda i: dists[i], reverse=True)[:k]\n",
        "    return [pairs[i] for i in top_idx]"
      ],
      "metadata": {
        "id": "MTh7lsXRKzeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def edge_upgrade_recommender(G: nx.DiGraph, *, k: int = 5) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Suggest *k* new edges that would most reduce diameter/ASPL (greedy heuristic).\"\"\"\n",
        "    UG = G.to_undirected()\n",
        "    base_diam = nx.diameter(_largest_component_subgraph(G))\n",
        "    base_aspl = nx.average_shortest_path_length(_largest_component_subgraph(G))\n",
        "\n",
        "    suggestions: List[Dict[str, Any]] = []\n",
        "    for (u, v) in _sample_longest_pairs(G, k * 3):\n",
        "        if UG.has_edge(u, v):\n",
        "            continue\n",
        "        H = UG.copy()\n",
        "        H.add_edge(u, v)\n",
        "        try:\n",
        "            new_diam = nx.diameter(_largest_component_subgraph(H))\n",
        "            new_aspl = nx.average_shortest_path_length(_largest_component_subgraph(H))\n",
        "        except nx.NetworkXError:\n",
        "            continue\n",
        "        suggestions.append({\n",
        "            \"edge\": (u, v),\n",
        "            \"delta_diameter\": base_diam - new_diam,\n",
        "            \"delta_aspl\": round(base_aspl - new_aspl, 4),\n",
        "        })\n",
        "    # Pick top‑k by ∆‑diameter then ∆‑ASPL\n",
        "    return sorted(suggestions, key=lambda x: (x[\"delta_diameter\"], x[\"delta_aspl\"]), reverse=True)[:k]"
      ],
      "metadata": {
        "id": "6ies7uLWK7lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def peering_impact_analyzer(\n",
        "    G: nx.DiGraph,\n",
        "    a: str | int,\n",
        "    b: str | int,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Evaluate topological impact of **adding** an undirected edge (a,b).\"\"\"\n",
        "    UG = G.to_undirected()\n",
        "    if UG.has_edge(a, b):\n",
        "        raise ValueError(\"Edge already exists – impact is zero.\")\n",
        "\n",
        "    base_diam = nx.diameter(_largest_component_subgraph(UG))\n",
        "    base_aspl = nx.average_shortest_path_length(_largest_component_subgraph(UG))\n",
        "\n",
        "    H = UG.copy()\n",
        "    H.add_edge(a, b)\n",
        "    new_diam = nx.diameter(_largest_component_subgraph(H))\n",
        "    new_aspl = nx.average_shortest_path_length(_largest_component_subgraph(H))\n",
        "\n",
        "    betw_before = nx.edge_betweenness_centrality(UG)\n",
        "    betw_after = nx.edge_betweenness_centrality(H)\n",
        "\n",
        "    # Sum absolute change as a proxy of load redistribution\n",
        "    load_shift = sum(abs(betw_after[e] - betw_before.get(e, 0)) for e in betw_after)\n",
        "\n",
        "    return {\n",
        "        \"delta_diameter\": base_diam - new_diam,\n",
        "        \"delta_aspl\": round(base_aspl - new_aspl, 4),\n",
        "        \"load_redistribution_score\": round(load_shift, 6),\n",
        "    }\n"
      ],
      "metadata": {
        "id": "1IknLgVbK7nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def redundancy_heatmap(G: nx.DiGraph, *, sample: int = 2000) -> Dict[int, int]:\n",
        "    \"\"\"Return histogram of edge‑disjoint path counts between random pairs.\"\"\"\n",
        "    UG = G.to_undirected()\n",
        "    hist: Counter[int] = Counter()\n",
        "    nodes = list(UG)\n",
        "    for _ in range(min(sample, len(nodes) ** 2)):\n",
        "        s, t = random.sample(nodes, 2)\n",
        "        try:\n",
        "            k = nx.edge_connectivity(UG, s, t)\n",
        "            hist[k] += 1\n",
        "        except nx.NetworkXError:\n",
        "            hist[0] += 1\n",
        "    return dict(hist)"
      ],
      "metadata": {
        "id": "9hBjwm5bK7pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def viral_failure_simulator(\n",
        "    G: nx.DiGraph,\n",
        "    *,\n",
        "    p: float = 0.1,\n",
        "    runs: int = 50,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Random‑failure percolation: remove each node with prob *p* (×runs).\"\"\"\n",
        "    surviving: List[float] = []\n",
        "    for _ in range(runs):\n",
        "        H = G.copy()\n",
        "        for n in list(H.nodes()):\n",
        "            if random.random() < p:\n",
        "                H.remove_node(n)\n",
        "        largest = _largest_component_subgraph(H) if H.number_of_nodes() else None\n",
        "        surviving.append(0 if largest is None else len(largest) / G.number_of_nodes())\n",
        "    return {\n",
        "        \"p\": p,\n",
        "        \"runs\": runs,\n",
        "        \"avg_survival_fraction\": round(sum(surviving) / len(surviving), 4),\n",
        "        \"values\": surviving,\n",
        "    }"
      ],
      "metadata": {
        "id": "ELlvT27UK7rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _power_law_alpha(values: Sequence[int | float]) -> float:\n",
        "    \"\"\"Rough power‑law exponent via log‑log CCDF slope.\"\"\"\n",
        "    vals = [v for v in values if v > 0]\n",
        "    if len(vals) < 10:\n",
        "        return float(\"nan\")\n",
        "    vals.sort()\n",
        "    ranks = range(1, len(vals) + 1)\n",
        "    xs = [math.log(v) for v in vals]\n",
        "    ys = [math.log(1 - (r - 1) / len(vals)) for r in ranks]\n",
        "    n = len(xs)\n",
        "    x_bar, y_bar = sum(xs) / n, sum(ys) / n\n",
        "    num = sum((x - x_bar) * (y - y_bar) for x, y in zip(xs, ys))\n",
        "    den = sum((x - x_bar) ** 2 for x in xs)\n",
        "    slope = num / den if den else float(\"nan\")\n",
        "    return -slope"
      ],
      "metadata": {
        "id": "aWI0F3ZiLFGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def degree_tail_watcher(graph_ts: List[Tuple[str, nx.DiGraph]]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Track power‑law α over time – each entry (timestamp, Graph).\"\"\"\n",
        "    series = []\n",
        "    for ts, G in graph_ts:\n",
        "        degs = [G.in_degree(n) + G.out_degree(n) if G.is_directed() else G.degree(n) for n in G]\n",
        "        series.append({\"timestamp\": ts, \"alpha\": _power_law_alpha(degs)})\n",
        "    return series"
      ],
      "metadata": {
        "id": "CMYPio7NLFIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spanner_extractor(\n",
        "    G: nx.DiGraph,\n",
        "    *,\n",
        "    eps: float = 0.2,\n",
        ") -> nx.Graph:\n",
        "    \"\"\"Return a (1+ε) greedy spanner using a simple distance‑preserving heuristic.\"\"\"\n",
        "    UG = G.to_undirected()\n",
        "    S = nx.Graph()\n",
        "    S.add_nodes_from(UG.nodes())\n",
        "    # Sort edges by weight 1 (all equal) for determinism\n",
        "    for u, v in UG.edges():\n",
        "        if u == v:\n",
        "            continue\n",
        "        if not nx.has_path(S, u, v):\n",
        "            S.add_edge(u, v)\n",
        "        else:\n",
        "            d_S = nx.shortest_path_length(S, u, v)\n",
        "            if d_S > (1 + eps):\n",
        "                S.add_edge(u, v)\n",
        "    return S"
      ],
      "metadata": {
        "id": "winDkREQLFJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spectral_partition_planner(G: nx.DiGraph, *, target_clusters: int = 4) -> Dict[str, Any]:\n",
        "    \"\"\"Partition graph into *target_clusters* via leading eigenvector method.\"\"\"\n",
        "    communities = list(nx.algorithms.community.leading_eigenvector_communities(G.to_undirected()))\n",
        "    # If we got more than needed – merge smallest sets until count matches.\n",
        "    while len(communities) > target_clusters:\n",
        "        # Merge two smallest\n",
        "        communities = sorted(communities, key=len)\n",
        "        merged = communities[0] | communities[1]\n",
        "        communities = [merged, *communities[2:]]\n",
        "    mapping = {n: i for i, c in enumerate(communities) for n in c}\n",
        "    Q = nx.algorithms.community.modularity(G.to_undirected(), communities)\n",
        "    return {\"communities\": mapping, \"modularity\": Q}"
      ],
      "metadata": {
        "id": "KQQcWnqKLLJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def congestion_proxy_rank(G: nx.DiGraph, *, top_n: int = 50) -> List[Tuple[Tuple[str, str], float]]:\n",
        "    \"\"\"Return *top_n* edges by edge‑betweenness centrality (proxy for load).\"\"\"\n",
        "    betw = nx.edge_betweenness_centrality(G)\n",
        "    return sorted(betw.items(), key=lambda x: x[1], reverse=True)[:top_n]"
      ],
      "metadata": {
        "id": "QGdkYRNYLLLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def k_augmentation_planner(\n",
        "    G: nx.DiGraph,\n",
        "    *,\n",
        "    k: int = 3,\n",
        "    candidate_set: Iterable[str] | None = None,\n",
        "    attach_degree: int = 5,\n",
        ") -> Dict[str, List[Tuple[str, str]]]:\n",
        "    \"\"\"Suggest how to attach *k* **new nodes** (IXP/POP) for max diameter gain.\n",
        "\n",
        "    Returns mapping: new_node → list of edges (new_node ↔ existing).\n",
        "    \"\"\"\n",
        "    UG = G.to_undirected()\n",
        "    if candidate_set is None:\n",
        "        candidate_set = [f\"NEW{i}\" for i in range(k)]\n",
        "    high_deg_nodes = sorted(UG.nodes(), key=UG.degree, reverse=True)[: attach_degree]\n",
        "\n",
        "    plan: Dict[str, List[Tuple[str, str]]] = {}\n",
        "\n",
        "    for new_node in list(candidate_set)[:k]:\n",
        "        plan[new_node] = [(new_node, hub) for hub in high_deg_nodes]\n",
        "    return plan"
      ],
      "metadata": {
        "id": "2cpfDk6mLLMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def meshedness_index(G: nx.DiGraph) -> float:\n",
        "    \"\"\"Compute (E − V + 1)/(2V − 5) – 0 for tree, 1 for maximally meshed planar.\"\"\"\n",
        "    V = G.number_of_nodes()\n",
        "    E = G.to_undirected().number_of_edges()\n",
        "    if V <= 2:\n",
        "        return 0.0\n",
        "    return (E - V + 1) / (2 * V - 5)"
      ],
      "metadata": {
        "id": "ou5zG3h3LR9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_ego_subgraph(\n",
        "    G: nx.DiGraph,\n",
        "    asn: str | int,\n",
        "    *,\n",
        "    radius: int = 1,\n",
        "    direction: str = \"both\",\n",
        ") -> nx.DiGraph:\n",
        "    \"\"\"Return the induced subgraph of all nodes within *radius* hops of *asn*.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    direction : {'in', 'out', 'both'}\n",
        "        • 'in'   – traverse incoming edges only (upstream view).\n",
        "        • 'out'  – traverse outgoing edges only (downstream view).\n",
        "        • 'both' – ignore direction (default).\n",
        "    \"\"\"\n",
        "    if direction not in {\"in\", \"out\", \"both\"}:\n",
        "        raise ValueError(\"direction must be 'in', 'out', or 'both'\")\n",
        "\n",
        "    if direction == \"both\" or not G.is_directed():\n",
        "        UG = G.to_undirected()\n",
        "        nodes = nx.single_source_shortest_path_length(UG, asn, cutoff=radius).keys()\n",
        "    else:\n",
        "        nbrs: Set[str | int] = {asn}\n",
        "        frontier: Set[str | int] = {asn}\n",
        "        for _ in range(radius):\n",
        "            next_frontier: Set[str | int] = set()\n",
        "            for n in frontier:\n",
        "                edges = G.in_edges(n) if direction == \"in\" else G.out_edges(n)\n",
        "                next_frontier.update(v if direction == \"in\" else w for v, w in edges)\n",
        "            nbrs.update(next_frontier)\n",
        "            frontier = next_frontier\n",
        "        nodes = nbrs\n",
        "    return G.subgraph(nodes).copy()"
      ],
      "metadata": {
        "id": "GNXhgj2RM7bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_customer_cone_subgraph(G: nx.DiGraph, asn: str | int) -> nx.DiGraph:\n",
        "    \"\"\"Return all ASNs **reachable downstream** from *asn* via provider→customer edges.\"\"\"\n",
        "    cone: Set[str | int] = set()\n",
        "    stack = [asn]\n",
        "    while stack:\n",
        "        current = stack.pop()\n",
        "        for _, child, data in G.out_edges(current, data=True):\n",
        "            if data.get(\"relation\") == \"p2c\" and child not in cone:\n",
        "                cone.add(child)\n",
        "                stack.append(child)\n",
        "    cone.add(asn)\n",
        "    return G.subgraph(cone).copy()"
      ],
      "metadata": {
        "id": "9MJp7X1IM7c9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_upstream_chain_subgraph(G: nx.DiGraph, asn: str | int, depth: int | None = None) -> nx.DiGraph:\n",
        "    \"\"\"Return chain of providers *upwards* from *asn* up to *depth* hops (None = unlimited).\"\"\"\n",
        "    chain: Set[str | int] = {asn}\n",
        "    current_level = {asn}\n",
        "    hops = 0\n",
        "    while current_level and (depth is None or hops < depth):\n",
        "        next_level: Set[str | int] = set()\n",
        "        for node in current_level:\n",
        "            for parent, _, data in G.in_edges(node, data=True):\n",
        "                if data.get(\"relation\") == \"p2c\" and parent not in chain:\n",
        "                    chain.add(parent)\n",
        "                    next_level.add(parent)\n",
        "        current_level = next_level\n",
        "        hops += 1\n",
        "    return G.subgraph(chain).copy()"
      ],
      "metadata": {
        "id": "E61pp6gKM7fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_peering_cluster_subgraph(\n",
        "    G: nx.DiGraph,\n",
        "    asn: str | int,\n",
        "    *,\n",
        "    include_peers_of_peers: bool = False,\n",
        ") -> nx.DiGraph:\n",
        "    \"\"\"Return subgraph of *asn* and all its **p2p peers** (optionally peers‑of‑peers).\"\"\"\n",
        "    peers: Set[str | int] = {\n",
        "        nbr\n",
        "        for nbr in G.neighbors(asn)\n",
        "        if G.get_edge_data(asn, nbr).get(\"relation\") == \"p2p\"\n",
        "    }\n",
        "    if include_peers_of_peers:\n",
        "        second_hop = set()\n",
        "        for p in peers:\n",
        "            second_hop.update(\n",
        "                n\n",
        "                for n in G.neighbors(p)\n",
        "                if G.get_edge_data(p, n).get(\"relation\") == \"p2p\"\n",
        "            )\n",
        "        peers.update(second_hop)\n",
        "    nodes = peers | {asn}\n",
        "    return G.subgraph(nodes).copy()"
      ],
      "metadata": {
        "id": "jT33nexHM7gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def induced_subgraph_by_asns(\n",
        "    G: nx.DiGraph,\n",
        "    asns: Iterable[str | int],\n",
        "    *,\n",
        "    keep_isolated: bool = False,\n",
        ") -> nx.DiGraph:\n",
        "    \"\"\"Return the subgraph induced by *asns*.\n",
        "\n",
        "    If *keep_isolated* is False, nodes with degree 0 inside the subset are\n",
        "    removed to focus the analysis.\n",
        "    \"\"\"\n",
        "    H = G.subgraph(asns).copy()\n",
        "    if not keep_isolated:\n",
        "        iso = list(nx.isolates(H))\n",
        "        H.remove_nodes_from(iso)\n",
        "    return H"
      ],
      "metadata": {
        "id": "0Dmz6vPgNEry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_betweenness_edge_subgraph(G: nx.DiGraph, top_k: int = 500) -> nx.DiGraph:\n",
        "    \"\"\"Return subgraph composed of the *top_k* edges by betweenness centrality.\"\"\"\n",
        "    betw = nx.edge_betweenness_centrality(G)\n",
        "    top_edges = [e for e, _ in sorted(betw.items(), key=lambda x: x[1], reverse=True)[:top_k]]\n",
        "    H = nx.DiGraph()\n",
        "    H.add_nodes_from(G.nodes(data=True))\n",
        "    for u, v in top_edges:\n",
        "        H.add_edge(u, v, **G.get_edge_data(u, v))\n",
        "    # Remove isolated nodes\n",
        "    H.remove_nodes_from(list(nx.isolates(H)))\n",
        "    return H"
      ],
      "metadata": {
        "id": "-la52rGLNG4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_ego_as_graph(\n",
        "    seed_asns: list[str | int],\n",
        "    *,\n",
        "    radius: int = 2,\n",
        "    from_time: str,\n",
        "    until_time: str,\n",
        "    collectors: list[str] | None = None,\n",
        "    record_type: str = \"ribs\",\n",
        "    rel_file: str | None = None,\n",
        ") -> nx.DiGraph:\n",
        "    \"\"\"Stream BGP paths but **retain only edges** within *radius* hops of *seed_asns*.\n",
        "\n",
        "    Rationale: ideal for engineers who want a quick neighbourhood view around\n",
        "    their own AS(es) without parsing millions of lines unrelated to them.\n",
        "    \"\"\"\n",
        "    collectors = collectors or [\"rrc00\"]\n",
        "    rels = load_caida_relationships(rel_file)\n",
        "\n",
        "    # Keep track of nodes within <= radius via incremental discovery.\n",
        "    discovered: Set[str] = set(map(str, seed_asns))\n",
        "\n",
        "    G = nx.DiGraph()\n",
        "    stream = pybgpstream.BGPStream(\n",
        "        from_time=from_time,\n",
        "        until_time=until_time,\n",
        "        collectors=collectors,\n",
        "        record_type=record_type,\n",
        "    )\n",
        "    for rec in stream.records():\n",
        "        for elem in rec:\n",
        "            hops = [k for k, _ in groupby(elem.fields[\"as-path\"].split())]\n",
        "            intersect = discovered.intersection(hops)\n",
        "            if not intersect:\n",
        "                continue  # skip paths outside interest zone\n",
        "\n",
        "            # Add edges but only for nodes within radius of any seed\n",
        "            for idx, (u, v) in enumerate(zip(hops[:-1], hops[1:])):\n",
        "                if any(abs(idx - hops.index(s)) <= radius for s in intersect):\n",
        "                    rel = rels.get((u, v), \"unknown\")\n",
        "                    if rel == \"p2c\":\n",
        "                        G.add_edge(u, v, relation=\"p2c\")\n",
        "                    elif rel in {\"p2p\", \"sibling\"}:\n",
        "                        G.add_edge(u, v, relation=rel)\n",
        "                        G.add_edge(v, u, relation=rel)\n",
        "                    else:\n",
        "                        G.add_edge(u, v, relation=\"unknown\")\n",
        "                        G.add_edge(v, u, relation=\"unknown\")\n",
        "                    discovered.update([u, v])\n",
        "    return G\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 4.2  Provider→customer cone builder  (downstream tree)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def build_customer_cone_graph(\n",
        "    root_asns: list[str | int],\n",
        "    *,\n",
        "    max_depth: int | None = None,\n",
        "    rel_file: str | None = None,\n",
        ") -> nx.DiGraph:\n",
        "    \"\"\"Construct a **provider→customer** tree using only CAIDA *as‑rel* file.\n",
        "\n",
        "    No BGPStream needed – therefore instantaneous.  Engineers often care about\n",
        "    the potential blast‑radius of their announcements; this tree provides it.\n",
        "    \"\"\"\n",
        "    rels = load_caida_relationships(rel_file)\n",
        "    # Build one‑directional DiGraph from relations alone\n",
        "    R = nx.DiGraph()\n",
        "    for (a, b), rel in rels.items():\n",
        "        if rel == \"p2c\":\n",
        "            R.add_edge(a, b, relation=\"p2c\")\n",
        "    cone_nodes: Set[str] = set(map(str, root_asns))\n",
        "    frontier: Set[str] = set(cone_nodes)\n",
        "    depth = 0\n",
        "    while frontier and (max_depth is None or depth < max_depth):\n",
        "        next_frontier: Set[str] = set()\n",
        "        for p in frontier:\n",
        "            for _, c in R.out_edges(p):\n",
        "                if c not in cone_nodes:\n",
        "                    cone_nodes.add(c)\n",
        "                    next_frontier.add(c)\n",
        "        frontier = next_frontier\n",
        "        depth += 1\n",
        "    return R.subgraph(cone_nodes).copy()\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 4.3  Fixed‑budget path sampler (fast snapshot)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def build_sampled_as_graph(\n",
        "    *,\n",
        "    path_limit: int = 100000,\n",
        "    from_time: str,\n",
        "    until_time: str,\n",
        "    collectors: list[str] | None = None,\n",
        "    record_type: str = \"updates\",\n",
        "    rel_file: str | None = None,\n",
        ") -> nx.DiGraph:\n",
        "    \"\"\"Stream only the **first *path_limit*** BGP elements and build the graph.\n",
        "\n",
        "    Useful when you want a *quick & dirty* Internet snapshot (<10 seconds) for\n",
        "    sanity‑checks, regression tests, or teaching demos.\n",
        "    \"\"\"\n",
        "    collectors = collectors or [\"rrc00\"]\n",
        "    rels = load_caida_relationships(rel_file)\n",
        "\n",
        "    G = nx.DiGraph()\n",
        "    stream = pybgpstream.BGPStream(\n",
        "        from_time=from_time,\n",
        "        until_time=until_time,\n",
        "        collectors=collectors,\n",
        "        record_type=record_type,\n",
        "    )\n",
        "    element_iter = islice((elem for rec in stream.records() for elem in rec), path_limit)\n",
        "    for elem in element_iter:\n",
        "        hops = [k for k, _ in groupby(elem.fields[\"as-path\"].split())]\n",
        "        for u, v in zip(hops[:-1], hops[1:]):\n",
        "            rel = rels.get((u, v), \"unknown\")\n",
        "            if rel == \"p2c\":\n",
        "                G.add_edge(u, v, relation=\"p2c\")\n",
        "            elif rel in {\"p2p\", \"sibling\"}:\n",
        "                G.add_edge(u, v, relation=rel)\n",
        "                G.add_edge(v, u, relation=rel)\n",
        "            else:\n",
        "                G.add_edge(u, v, relation=\"unknown\")\n",
        "                G.add_edge(v, u, relation=\"unknown\")\n",
        "    return G\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 4.4  Two‑collector diff builder (regional dx)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def build_diff_collectors_graph(\n",
        "    *,\n",
        "    collectors_a: list[str],\n",
        "    collectors_b: list[str],\n",
        "    from_time: str,\n",
        "    until_time: str,\n",
        "    record_type: str = \"ribs\",\n",
        "    rel_file: str | None = None,\n",
        ") -> Tuple[nx.DiGraph, nx.DiGraph, nx.DiGraph]:\n",
        "    \"\"\"Build two graphs from **distinct collector sets** and return their *diff*.\n",
        "\n",
        "    Engineers can spot region‑specific links that appear only in one view.\n",
        "    Returns  (G_a, G_b, G_diff) where G_diff = symmetric difference of edges.\n",
        "    \"\"\"\n",
        "    G_a = create_bgpstream_as_rel_graph(\n",
        "        from_time, until_time, collectors_a, record_type, rel_file\n",
        "    )[0]\n",
        "    G_b = create_bgpstream_as_rel_graph(\n",
        "        from_time, until_time, collectors_b, record_type, rel_file\n",
        "    )[0]\n",
        "    diff = nx.DiGraph()\n",
        "    edges_a = set(G_a.edges())\n",
        "    edges_b = set(G_b.edges())\n",
        "    for u, v in edges_a.symmetric_difference(edges_b):\n",
        "        # Attach direction from whichever graph had it\n",
        "        if (u, v) in edges_a:\n",
        "            diff.add_edge(u, v, **G_a.get_edge_data(u, v))\n",
        "        else:\n",
        "            diff.add_edge(u, v, **G_b.get_edge_data(u, v))\n",
        "    return G_a, G_b, diff"
      ],
      "metadata": {
        "id": "0mMte-4gO634"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# More tools:\n",
        "# Instead of numbers nodes - create AS object and use it as node, with all its properties"
      ],
      "metadata": {
        "id": "W-_bjjMCVxFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RPKI Views**"
      ],
      "metadata": {
        "id": "rq4J5VZOmRBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tools.rpki_views.rpkiviews_tools import *\n",
        "from tools.rpki_views.rpkiviews_aux import *"
      ],
      "metadata": {
        "id": "vpY9Z3VEmUdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rpkiviews_prefixes_asns('31.168.36.0/23')"
      ],
      "metadata": {
        "id": "H2LrV2vjmcj1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}